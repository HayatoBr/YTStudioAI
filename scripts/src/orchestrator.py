from __future__ import annotations


def _pick_music_path(root: str) -> str | None:
    """
    Escolhe uma trilha de fundo se existir em assets/music.
    Prioridade: bg.mp3, depois qualquer arquivo de Ã¡udio.
    """
    music_dir = os.path.join(root, "assets", "music")
    if not os.path.isdir(music_dir):
        return None
    preferred = os.path.join(music_dir, "bg.mp3")
    if os.path.exists(preferred):
        return preferred
    for name in os.listdir(music_dir):
        low = name.lower()
        if low.endswith((".mp3", ".wav", ".m4a", ".aac", ".ogg")):
            p = os.path.join(music_dir, name)
            if os.path.isfile(p):
                return p
    return None


def _encode_voice_to_m4a(voice_mp3: str, out_m4a: str, duration_sec: float) -> None:
    """
    Encode simples (sem trilha) para m4a AAC.
    """
    ff = ensure_ffmpeg() if "ensure_ffmpeg" in globals() else None
    ff = ff or os.getenv("FFMPEG_PATH") or "ffmpeg"
    cmd = [
        ff, "-y",
        "-i", voice_mp3,
        "-t", f"{float(duration_sec):.3f}",
        "-c:a", "aac",
        "-b:a", "256k",
        "-movflags", "+faststart",
        "-loglevel", "error",
        out_m4a,
    ]
    run_ffmpeg_with_progress(cmd, total_duration_sec=float(duration_sec), label="Encodando voz (sem trilha)")

import os
import json
from typing import Dict, Any, Union

from scripts.src.script_provider import generate_short_script, generate_long_script
from scripts.src.tts_openai import generate_tts_mp3
from scripts.src.audio_mix import mix_voice_with_music
from scripts.src.renderer import render_short_video, render_long_video_16x9, render_long_video_9x16
from scripts.src.ffmpeg_tools import get_media_duration_seconds
from scripts.src.subtitle_validator import validate_subtitles
from scripts.src.subtitle_from_script import apply_subtitles_from_script

# Compat: visual_extractor teve nomes diferentes ao longo dos patches
import scripts.src.visual_extractor as _ve

def _get_build_visual_plan():
    for name in (
        "build_visual_plan",
        "build_plan",
        "build_visuals",
        "make_visual_plan",
        "create_visual_plan",
        "generate_visual_plan",
        "extract_visual_plan",
        "plan_visuals",
    ):
        fn = getattr(_ve, name, None)
        if callable(fn):
            return fn
    return lambda data: data

_build_visual_plan = _get_build_visual_plan()

def _ensure_dict(data: Union[str, Dict[str, Any]]) -> Dict[str, Any]:
    """Garante que o resultado do gerador seja um dict."""
    if isinstance(data, dict):
        return data
    if isinstance(data, str):
        s = data.strip()
        # tenta JSON direto
        if s.startswith("{") and s.endswith("}"):
            try:
                obj = json.loads(s)
                if isinstance(obj, dict):
                    return obj
            except Exception:
                pass
        # tenta extrair bloco JSON dentro do texto
        try:
            start = s.find("{")
            end = s.rfind("}")
            if start != -1 and end != -1 and end > start:
                obj = json.loads(s[start:end+1])
                if isinstance(obj, dict):
                    return obj
        except Exception:
            pass
        # fallback: trata como narraÃ§Ã£o pura
        return {
            "title": "Arquivo Oculto (auto)",
            "narration": s,
            "scenes": [{"scene_id": 1, "subtitle_chunks": ["â€¦"]}],
        }
    # fallback
    return {
        "title": "Arquivo Oculto (auto)",
        "narration": "",
        "scenes": [{"scene_id": 1, "subtitle_chunks": ["â€¦"]}],
    }

def _ensure_scenes(short_data: Dict[str, Any]) -> None:
    scenes = short_data.get("scenes")
    if not isinstance(scenes, list) or not scenes:
        short_data["scenes"] = [{"scene_id": 1, "subtitle_chunks": ["â€¦"]}]

def run_auto_short() -> Dict[str, Any]:
    root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))

    requested_duration_sec = float(os.getenv("AO_SHORT_SECONDS", "55"))
    # Por padrÃ£o, usamos a duraÃ§Ã£o REAL do Ã¡udio (evita drift de legendas).
    # Se quiser forÃ§ar exatamente AO_SHORT_SECONDS, defina AO_FORCE_SHORT_SECONDS=1.
    duration_sec = requested_duration_sec
    print(f"â–¶ Gerando SHORT ({int(requested_duration_sec)}s) em modo automÃ¡tico...")

    print("ðŸ§  Gerando roteiro automÃ¡tico...")
    short_data_raw = generate_short_script()
    short_data = _ensure_dict(short_data_raw)
    _ensure_scenes(short_data)

    narration_text = str(short_data.get("narration") or "").strip()

    # âœ… Legendas DEVEM vir da narraÃ§Ã£o (nÃ£o do plano visual)
    scenes = short_data.get("scenes") or []
    if isinstance(scenes, list) and narration_text:
        apply_subtitles_from_script(scenes, narration_text, max_chars=int(os.getenv("AO_SUB_MAX_CHARS", "30")))
        short_data["scenes"] = scenes

    validate_subtitles(short_data, strict=os.getenv("AO_SUBS_STRICT", "0") == "1")

    # Plano visual Ã© para imagens/movimento (nÃ£o para texto das legendas)
    short_data = _build_visual_plan(short_data)
    try:
        print(f"ðŸ§© Plano visual: {len(short_data.get('scenes', []))} cenas")
    except Exception:
        print("ðŸ§© Plano visual: ok")

    out_audio_dir = os.path.join(root, "output", "audio")
    os.makedirs(out_audio_dir, exist_ok=True)
    voice_path = os.path.join(out_audio_dir, "voice.mp3")
    mixed_path = os.path.join(out_audio_dir, "mixed.m4a")

    print("ðŸŽ™ï¸ Gerando narraÃ§Ã£o (OpenAI TTS)...")
    generate_tts_mp3(
        narration_text,
        voice_path,
        voice=os.getenv("AO_TTS_VOICE", "cedar"),
        speed=float(os.getenv("AO_TTS_SPEED", "1.0")),
    )

    # Mede duraÃ§Ã£o real da narraÃ§Ã£o para sincronizar legendas/render com o Ã¡udio
    try:
        voice_dur = float(get_media_duration_seconds(voice_path))
    except Exception:
        voice_dur = None

    end_pad = float(os.getenv("AO_END_PAD_SEC", "0.25"))
    if voice_dur and voice_dur > 0:
        measured = voice_dur + max(0.0, end_pad)
        if os.getenv("AO_FORCE_SHORT_SECONDS", "0") == "1":
            duration_sec = float(requested_duration_sec)
        else:
            duration_sec = measured
    

    print("ðŸŽšï¸ Mixando voz + trilha (ducking)...")
    mix_voice_with_music(voice_path, mixed_path, duration_sec=duration_sec)

    short_data["_audio_path"] = mixed_path

    print("ðŸŽ¬ Renderizando vÃ­deo SHORT...")
    out_video = render_short_video(short_data, duration_sec=duration_sec)
    print(f"âœ… SHORT finalizado!\nðŸ“„ VÃ­deo: {out_video}")
    return {"video": out_video, "audio": mixed_path}


def run_auto_long(minutes: float | None = None) -> Dict[str, Any]:
    """
    Pipeline LONG automÃ¡tico (duraÃ§Ã£o alvo via --minutes):
    - Roteiro LONG (JSON)
    - Legendas extraÃ­das da narraÃ§Ã£o
    - Visual plan (imagens/motion)
    - TTS + mix (ducking)
    - Render 16:9 e 9:16
    """
    root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))

    print("â–¶ Gerando LONG em modo automÃ¡tico...")
    print("ðŸ§  Gerando roteiro LONG automÃ¡tico...")
    long_data = _ensure_dict(generate_long_script(target_minutes=minutes))

    narration_text = str(long_data.get("narration") or "").strip()
    if not narration_text:
        raise RuntimeError("Roteiro LONG veio sem 'narration'.")

    scenes = long_data.get("scenes") or []
    apply_subtitles_from_script(
        scenes,
        narration_text,
        max_chars=int(os.getenv("AO_SUB_MAX_CHARS", "32")),
    )
    long_data["scenes"] = scenes

    validate_subtitles(long_data, strict=os.getenv("AO_SUBS_STRICT", "0") == "1")

    # Visual plan
    if os.getenv("AO_IMAGES_ENABLED", "1") == "1":
        try:
            long_data = _build_visual_plan(long_data)
            try:
                print(f"ðŸ§© Plano visual: {len(long_data.get('scenes', []))} cenas")
            except Exception:
                print("ðŸ§© Plano visual: ok")
        except Exception as e:
            print(f"âš ï¸ Falha ao gerar imagens (continuando sem imagens): {e}")

    out_audio_dir = os.path.join(root, "output", "audio")
    os.makedirs(out_audio_dir, exist_ok=True)

    voice_path = os.path.join(out_audio_dir, "voice_long.mp3")
    mixed_path = os.path.join(out_audio_dir, "mixed_long.m4a")

    print("ðŸŽ™ï¸ Gerando narraÃ§Ã£o LONG (OpenAI TTS)...")
    generate_tts_mp3(
        narration_text,
        voice_path,
        voice=os.getenv("AO_TTS_VOICE", "cedar"),
        speed=float(os.getenv("AO_TTS_SPEED", "1.0")),
    )

    # duraÃ§Ã£o real da voz
    try:
        voice_dur = float(get_media_duration_seconds(voice_path))
        end_pad = float(os.getenv("AO_END_PAD_SEC", "0.35"))
        duration_sec = max(1.0, voice_dur + max(0.0, end_pad))
    except Exception:
        duration_sec = float(os.getenv("AO_LONG_FALLBACK_SECONDS", "420"))

    print("ðŸŽšï¸ Mixando voz + trilha (ducking)...")
    music_path = _pick_music_path(root)
    if music_path:
        mix_voice_with_music(
            voice_path=voice_path,
            music_path=music_path,
            out_path=mixed_path,
            duration_sec=duration_sec,
        )
    else:
        print("âš ï¸ Nenhuma trilha encontrada. Renderizando apenas com a voz.")
        _encode_voice_to_m4a(voice_path, mixed_path, duration_sec)

    # duraÃ§Ã£o final baseada no mix
    try:
        final_dur = float(get_media_duration_seconds(mixed_path))
        if final_dur and final_dur > 0:
            duration_sec = final_dur
    except Exception:
        pass

    long_data["_audio_path"] = mixed_path

    print("ðŸŽ¬ Renderizando vÃ­deo LONG (16:9 e 9:16)...")
    out_16x9 = render_long_video_16x9(long_data, duration_sec=duration_sec)
    out_9x16 = render_long_video_9x16(long_data, duration_sec=duration_sec)

    print("âœ… LONG finalizado!")
    print(f"ðŸ“„ 16:9: {out_16x9}")
    print(f"ðŸ“„ 9:16: {out_9x16}")

    return {"video_16x9": out_16x9, "video_9x16": out_9x16, "audio": mixed_path}

